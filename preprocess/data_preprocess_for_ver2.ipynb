{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/jlin/.pyenv/versions/kanjian/lib/python3.5/site-packages/jieba/dict.txt.big ...\n",
      "Dumping model to file cache /var/folders/35/zb7fx0vn48z5c3t8v8kmfv9r0000gn/T/jieba.uf957e97c453f115ab85d0b40b2015182.cache\n",
      "Loading model cost 3.203 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functions_v2\n",
    "from collections import Counter, defaultdict\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "source_path = '/Users/jlin/Lingtelli/kanjian/'\n",
    "data_path = source_path + 'data/'\n",
    "output_path = source_path + 'temp_cluster/'\n",
    "dataset_file = 'new_cluster_result02.csv'\n",
    "keyword_file = 'key_word_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractInfo(c):\n",
    "    searchObj = re.search( r'[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}(.*)$', c['comment_no_punc'], re.M|re.I)\n",
    "    if searchObj is None:\n",
    "        print('NO PATTERN MATCHED IN COMMENT %s' % c['comment_no_punc'])\n",
    "        c['article_id'] = 'NO_DATA'\n",
    "        c['tags'] = 'NO_DATA'\n",
    "        c['comment_pure'] = c['comment_no_punc']\n",
    "    else:\n",
    "        c['article_id'] = c['comment_no_punc'][searchObj.start(): searchObj.end()].split(' ', 1)[0]\n",
    "        c['tags'] = c['comment_no_punc'][searchObj.start(): searchObj.end()].split(' ', 1)[1]\n",
    "        c['comment_pure'] = c['comment_no_punc'][: searchObj.start()]                                # 沒有標點符號的版本段落\n",
    "    \n",
    "    search_content = re.search( r'[a-z0-9]{8}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{4}-[a-z0-9]{12}(.*)$', c['comment'], re.M|re.I)\n",
    "    if search_content is not None:\n",
    "        c['content'] = c['comment'][: search_content.start()]\n",
    "    else:\n",
    "        c['content'] = c['comment']\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# erase [nan], NaN in df_comment_tag['keywords']\n",
    "def cleanKeywords(keywords):\n",
    "    if (type(keywords) == float) or (len(keywords) > 0 and type(keywords[0]) is float):\n",
    "        return []\n",
    "    keywords = [x for x in keywords if str(x) != 'nan']\n",
    "    return list(set(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeTitle(data):\n",
    "    try:\n",
    "        if np.isnan(data['WEB_TITLE']):\n",
    "            return data['PAPER_TITLE']\n",
    "    except:\n",
    "        return data['WEB_TITLE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toDateForm(data):\n",
    "    date_str = data['PUBLISH_DATE']# if data['PUBLISH_DATE'] != 'NO_VALUE' else data['CREATE_TIME']\n",
    "    date = date_str.split(' ')[0]\n",
    "    date_split = date.split('/')\n",
    "    try:\n",
    "        if len(date_split[1]) == 1:\n",
    "            date_split[1] = '0' + date_split[1]\n",
    "        if len(date_split[2]) == 1:\n",
    "            date_split[2] = '0' + date_split[2]\n",
    "        return ''.join(date_split)\n",
    "    except:\n",
    "        print('cannot convert date format of', data['UUID'])\n",
    "        return date_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path + dataset_file, names=['count', 'comments'])\n",
    "df_keyword = pd.read_csv(data_path + keyword_file, sep='\\t', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = df_keyword['WORD'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in keywords:\n",
    "    if type(k) is not float:\n",
    "        jieba.add_word(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 把同一文章ID的WORD集合起來\n",
    "df_keyword = df_keyword.groupby('ALL_UNIT_UUID')['WORD'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[df['count'] >= 1].reset_index()\n",
    "df['comments'] = df['comments'].apply(functions_v2.strToSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "comments = list(set([c for comments in df['comments'].tolist() for c in comments]))\n",
    "df_comment = pd.DataFrame({'comment': comments})\n",
    "df_comment['comment_no_punc'] = df_comment['comment'].apply(functions_v2.removePunctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO PATTERN MATCHED IN COMMENT     functionw, d, s, m {        var f  d.getElementsByTagName'script'0\n",
      "NO PATTERN MATCHED IN COMMENT           j  d.createElement'script'\n"
     ]
    }
   ],
   "source": [
    "df_comment = df_comment.apply(extractInfo, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_comment_tag 內包含各文章所屬的文章id, tags: claude那裡分析出的關鍵字, WORD: 該文章給定的關鍵字, comment\n",
    "df_comment_tag = df_comment.merge(df_keyword, left_on='article_id', right_on='ALL_UNIT_UUID', how='left')\n",
    "df_comment_tag = df_comment_tag[['content', 'comment_pure', 'article_id', 'WORD']]\n",
    "df_comment_tag.rename(columns={'comment_pure': 'comment_no_punc', 'WORD': 'keywords'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_comment_tag['keywords'] = df_comment_tag['keywords'].apply(cleanKeywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_comment_tag['keywords_str'] = df_comment_tag['keywords'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_comment_tag['seg_comment'] = df_comment_tag['comment_no_punc'].apply(lambda x: jieba.lcut(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Article Information (Author, Publish Date, Title, Type, Subtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_article_info = pd.read_csv('~/Downloads/kangjiantype/20160715-ch-article_data.csv')\n",
    "df_article_info = df_article_info[pd.notnull(df_article_info['PUBLISH_DATE'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_article_info['TITLE'] = df_article_info.apply(makeTitle, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_article_info['PUBLISH_DATE'] = df_article_info.apply(toDateForm, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_article_subtype = pd.read_csv('/Users/jlin/Downloads/kangjiantype/0727-article_type_ref.csv')        # article_data_uuid, sub_type_id\n",
    "df_subtype_ref = pd.read_csv('/Users/jlin/Downloads/kangjiantype/0727-sub_type_code_utf8_clean.csv')    # 文章的子分類, 該子分類所屬的main_type\n",
    "df_type_ref = pd.read_csv('/Users/jlin/Downloads/kangjiantype/0727-type_code.csv', encoding='big5')     # 文章的分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所整理的資料共有 58 種子項目\n",
      "全部共有 429 個子項目\n",
      "沒有資料不在全部子項目裡？ False , 整理的資料多了 [nan]\n"
     ]
    }
   ],
   "source": [
    "# 抓到文章的子分類id\n",
    "df_article_info_subtype = df_article_info.merge(\n",
    "    df_article_subtype[['ARTICLE_DATA_UUID', 'SUB_TYPE']], left_on='UUID', right_on='ARTICLE_DATA_UUID', left_index=True, how='left')\n",
    "print('所整理的資料共有', len(df_article_info_subtype['SUB_TYPE'].unique()), '種子項目')\n",
    "print('全部共有', len(df_subtype_ref['IUUID'].unique()), '個子項目')\n",
    "print('沒有資料不在全部子項目裡？', set(df_subtype_ref['IUUID'].unique()) > set(df_article_info_subtype['SUB_TYPE']), \n",
    "      ', 整理的資料多了', list(set(df_article_info_subtype['SUB_TYPE']) - set(df_subtype_ref['IUUID'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 抓到子分類的命名以及分類的id\n",
    "df_article_info_subtype_name = df_article_info_subtype.merge(\n",
    "    df_subtype_ref[['IUUID', 'NAME', 'MAIN_TYPE']], left_on='SUB_TYPE', right_on='IUUID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 抓到分類的命名\n",
    "df_article_info_all_type = df_article_info_subtype_name.merge(\n",
    "    df_type_ref[['UUID', 'NAME']], left_on='MAIN_TYPE', right_on='UUID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# UUID_y: 文章分類項目的id, SUB_TYPE: 文章分類子項目的id\n",
    "df_article_info_all_type = df_article_info_all_type[['UUID_x', 'TITLE', 'AUTHOR', 'PUBLISH_DATE', 'URL_ID', 'NAME_x', 'NAME_y']]\n",
    "df_article_info_all_type.rename(\n",
    "    columns={'UUID_x': 'article_id', 'TITLE': 'title', 'AUTHOR':'author', 'PUBLISH_DATE':'publish_date', 'URL_ID':'url', 'NAME_x':'category', 'NAME_y':'type'},\n",
    "    inplace=True)\n",
    "# subtype is category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25736 7491\n",
      "11733 11173\n",
      "27914\n"
     ]
    }
   ],
   "source": [
    "print(len(df_comment_tag), len(df_comment_tag['article_id'].unique()))\n",
    "print(len(df_article_info_all_type), len(df_article_info_all_type['article_id'].unique()))\n",
    "df_dataset = df_comment_tag.merge(df_article_info_all_type, on='article_id', how='left')\n",
    "print(len(df_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dataset = df_dataset[\n",
    "    ['content', 'article_id', 'keywords', 'keywords_str', 'title', 'author', 'publish_date', 'url', 'category']\n",
    "].to_msgpack('df_dataset.msg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dataset = pd.read_msgpack('df_dataset.msg')\n",
    "df_dataset['keywords_str'] = df_dataset['keywords'].apply(lambda x: ' '.join(x))\n",
    "df_dataset['comment_no_punc'] = df_dataset['content'].apply(functions_v2.removePunctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        2016-07-01\n",
       "1        2012-03-01\n",
       "2        2016-05-24\n",
       "3        2010-01-01\n",
       "4        2012-05-01\n",
       "5        2011-01-01\n",
       "6        2011-11-01\n",
       "7        2011-11-01\n",
       "8        2015-03-01\n",
       "9        2015-08-10\n",
       "10       2000-11-01\n",
       "11       2015-03-04\n",
       "12       2011-05-27\n",
       "13       2015-11-05\n",
       "14       2015-10-30\n",
       "15       2013-09-01\n",
       "16       1998-11-01\n",
       "17       2002-04-01\n",
       "18       2016-04-27\n",
       "19       2013-02-01\n",
       "20       2009-06-01\n",
       "21       2016-02-01\n",
       "22       2009-06-01\n",
       "23       2011-07-13\n",
       "24       2011-07-13\n",
       "25       2014-08-01\n",
       "26       2014-11-01\n",
       "27       2007-04-01\n",
       "28       2012-01-01\n",
       "29       2016-01-13\n",
       "            ...    \n",
       "27884    2008-01-01\n",
       "27885    2008-06-01\n",
       "27886    2014-09-04\n",
       "27887    2010-09-01\n",
       "27888    2016-03-01\n",
       "27889    2007-02-01\n",
       "27890    2009-08-01\n",
       "27891    2009-08-01\n",
       "27892    2015-02-02\n",
       "27893    2013-01-01\n",
       "27894    2015-03-01\n",
       "27895    2010-04-01\n",
       "27896    2014-08-26\n",
       "27897    2012-10-01\n",
       "27898    2014-05-20\n",
       "27899    2001-12-01\n",
       "27900    2006-05-01\n",
       "27901    2005-07-01\n",
       "27902    2009-04-01\n",
       "27903    2014-09-04\n",
       "27904    2012-05-01\n",
       "27905    2011-08-08\n",
       "27906    2010-12-01\n",
       "27907    2014-10-01\n",
       "27908    2013-10-22\n",
       "27909    2011-03-27\n",
       "27910    2012-07-01\n",
       "27911    2015-11-01\n",
       "27912    2015-03-01\n",
       "27913    2016-01-25\n",
       "Name: publish_date, dtype: object"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset['publish_date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentences = df_comment_tag['seg_comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model = word2vec.Word2Vec(sentences,)\n",
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v_model.save('w2v_model.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model = word2vec.Word2Vec.load('w2v_model.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 很快的查詢方法，目前僅支援一個input\n",
    "#query_list的詞有出現在內文或關鍵字\n",
    "def keywordQuery(query_list, return_size=100):\n",
    "    query = '|'.join(query_list)\n",
    "    tdf = df_dataset[(df_dataset.comment.str.contains(query, na=False)) | (df_dataset.keywords.str.contains(query, na=False))]\n",
    "    keyword_contain_query = [k for kws in tdf.keywords.str for k in kws if (type(k) is not float) and (k not in query)]\n",
    "    return tdf, Counter(keyword_contain_query).most_common(return_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 利用word2vec查出與_query_list內所有query相近的詞與為一群\n",
    "def getConceptWords(words, threshold=0.65):\n",
    "    concepts = defaultdict(list)\n",
    "    for w in words:\n",
    "        #print('\\n\\n', w, '->', end=' ')\n",
    "        try:\n",
    "            most_similar = w2v_model.most_similar(w, topn=9)\n",
    "            concepts[w] = [m[0] for m in most_similar if m[1] >= threshold] + [w]\n",
    "        except:\n",
    "            continue\n",
    "        #print(concepts[w])\n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 該文章有這個keyword，但是該段落完全沒有類似的字眼，是否該回傳？ 還是回傳整段文章？\n",
    "# 把文章合併或是照文章編排\n",
    "def subkeywordQuery(query_list, concepts):\n",
    "    query = '|'.join(query_list)\n",
    "    tdf = df_dataset[(df_dataset.comment.str.contains(query, na=False)) | (df_dataset.keywords_str.str.contains(query, na=False))]\n",
    "    \n",
    "    for concept, words in concepts.items():\n",
    "        print(concept, 'size:', end='')\n",
    "        _query = '|'.join(words)\n",
    "        temp_df = tdf[tdf.keywords_str.str.contains(_query, na=False)]\n",
    "        print(len(temp_df))\n",
    "        print(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def allFieldsQuery(query_list, keywords=None, author=None, publish_date=None, categories=None, most_common_size=10, cluster_idx=0, batch_size=5):\n",
    "    query = '|'.join(query_list)\n",
    "    tdf = df_dataset[(df_dataset.comment_no_punc.str.contains(query, na=False)) | (df_dataset.keywords_str.str.contains(query, na=False))]\n",
    "    \n",
    "    keyword_contain_query = [k for kws in tdf.keywords.str for k in kws if (type(k) is not float) and (k not in query)]\n",
    "    top_keywords = [w[0] for w in Counter(keyword_contain_query).most_common(most_common_size)]\n",
    "\n",
    "    if author is not None:\n",
    "        tdf = tdf[(tdf.author.str.contains(author, na=False))]\n",
    "    \n",
    "    if categories is not None:\n",
    "        query = '|'.join(categories)\n",
    "        tdf = tdf[(tdf.category.str.contains(query, na=False))]\n",
    "    \n",
    "    if publish_date is not None:\n",
    "        start = ''.join(publish_date[0].split('-'))\n",
    "        end = ''.join(publish_date[1].split('-'))\n",
    "        mask = (tdf['publish_date'] >= start) & (tdf['publish_date'] <= end)\n",
    "        tdf = tdf.loc[mask]\n",
    "    \n",
    "    clusters = []\n",
    "    concept_list = []\n",
    "    if keywords is not None:\n",
    "        concepts = getConceptWords(keywords)\n",
    "    else:\n",
    "        concepts = getConceptWords(top_keywords[:most_common_size])\n",
    "        \n",
    "    for concept, words in concepts.items():\n",
    "        _query = '|'.join(words)\n",
    "        #print('\\n\\n', _query)\n",
    "        print(concept, 'size:', end='')\n",
    "        temp_df = tdf[tdf.keywords_str.str.contains(_query, na=False)]\n",
    "        print(len(temp_df))\n",
    "        if len(temp_df) > 0:\n",
    "            clusters.append(temp_df)\n",
    "            concept_list.append(concept)\n",
    "            #print(temp_df)\n",
    "    return clusters[cluster_idx: cluster_idx + batch_size], concept_list[cluster_idx: cluster_idx + batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convertJSON(df_list, keywords):\n",
    "    clusters = []\n",
    "    \n",
    "    for df, keyword in zip(df_list, keywords):\n",
    "        cluster_obj = dict()\n",
    "        cluster_obj['keyword'] = keyword\n",
    "        cluster_obj['size'] = len(df)\n",
    "        temp_df = df[['content', 'article_id', 'title', 'author', 'publish_date', 'url', 'category', 'keywords']]\n",
    "        cluster_obj['member'] = temp_df.to_dict(orient='records')\n",
    "        clusters.append(cluster_obj)\n",
    "        \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "義診 size:1\n",
      "安養機構 size:2\n",
      "醫療 size:1\n",
      "基改食品 size:1\n",
      "台灣 size:1\n",
      "火災 size:2\n",
      "高樓層 size:2\n",
      "消防 size:2\n"
     ]
    }
   ],
   "source": [
    "cc, c = allFieldsQuery(['水災'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters_json = convertJSON(cc, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keyword': '義診',\n",
       "  'member': [{'article_id': '065ece36-6a2e-4059-8a96-d09e90c9bc63',\n",
       "    'author': '林芝安',\n",
       "    'category': '醫療迷思',\n",
       "    'content': '清晨6點不到，所有團員在駐紮一晚的蒙古包外面集合，壓緊帽子、裹著禦寒大衣，逐一清點好相關急救醫療物品後，開始往看來不算艱困的山上挺進。 沒多久，才發現這座山會騙人，因為坡度不小，中途得休息好幾回，除了體力過人的領隊唐高駿一馬當先之外，大夥無不氣喘吁吁，餓著肚子繼續攻堅。在陣陣冷風中，總算攀上山頭，準備進行災難醫療救援訓練（Disaster Medical Assistant Team , DMAT）。 大夥各自就定位，有人扮演傷患、有人負責通報、有人忙著綁繩結、還有些人負責製作擔架……隨著分秒流逝，突然，朝陽從遙遠山頭探出臉來，頃刻間，第一道曙光射出，金色光芒遍灑山頭，然後將腳下的大地染成黃紅一片，在山上忙個不停的我們，有如沐浴在光中。 除了義診工作，接受災難醫療救援訓練也是本次義診團的主要任務之一，所有醫護人員都必須參與演習。 當災難發生時，如何在最短的時間內進行緊急救護與醫療照顧，將傷害減至最低，平時的訓練與演習就變得很重要，尤其台灣，地震、水災等各種災難頻繁，一旦發生重大災難，醫療救援隊必須緊急進駐，協助災區復原。 台北市立聯合醫院災難醫療救援隊利用這次外蒙義診，培訓醫護人員為種子志工，進行野外訓練。 就地取材，用枯木做擔架 台北市立聯合醫院忠孝院區院長兼總院區緊急醫療部部長唐高駿說，難得有機會移師外蒙古山區，可練習如何就地取材，利用當地既有的資源進行應變與救援。說著說著，唐高駿轉身往山坡樹林方向跑，沒多久，他肩上扛著長條白樺樹枯木回來，利用兩根木頭將三件救援背心串起來，就完成了克難擔架。 模擬事件：登山意外 醫護人員趕抵現場，檢傷後得知，一位重傷（疑似脊椎受傷）、兩位輕傷（一位手傷，一位腳傷）。 先在山上對重傷病患進行頸椎固定，使用繩結將患者綑綁在擔架上，初步評估有無心跳、血壓後，趕緊利用無線通報醫療站人員待命，之後用枯木做成的擔架將重傷患者搬運下山，另外兩位輕傷患者則用三角巾簡單固定傷口，由醫護人員陪同，緩步下山。 一抵達山下醫療站（設於蒙古包內），待命人員隨即對重傷患者進行高級創傷救命術（ATLS）及高級心臟救命術（ACLS）的評估，先評估病患的呼吸道是否順暢，再測量生命徵象（血壓、血氧），給予氧氣，建立靜脈輸液管路，並做神經學檢查，並針對緊急的問題做處理，穩定病患的生命徵象。 確認病患生命徵象穩定後，立即以無線電通報指揮派遣中心，連絡後送事宜。接著，將重傷病患送上救護車，緊急送往就近的醫院處置。 這是我們在外蒙古特勒吉山區的演練過程。充當重傷病患的我初體驗被綑綁在克難擔架上，由兩位急救人員抬下山，躺在擔架上不太舒服，腰背很痛，抬的人恐怕更辛苦，因為邊扛邊下坡。 為了這次外蒙訓練，出國前一星期的週末，唐高駿派義診成員忠孝院區護理科督導魏惠志與和平院區護士鄭雪琴，前往南投參加北區國家災難醫療救護隊野外模擬演習。 課程非常多元，包括如何搭設野外醫療站、檢傷分類及創傷處理、醫療站規劃設計、衛星定位系統（GPS）的使用、衛星電話的操作、繩結操作、野外求生及傷病患緊急醫療處置綜合演練等。 南投演習訓練的總指揮官是台大醫院急診醫學部主治醫師、台北區災難應變指揮中心（EOC）執行長石富元，他強調，災難醫學的重點是「緊急應變準備」，最重要的工作是準備，就像球隊，平常若沒勤奮練習，比賽前即使請國際級教練親臨指導也沒用。 災難醫療救援訓練也是如此，921大地震、風災、水災或SARS期間，醫療衛生體系都必須要做一些應變及進行初期救援或是醫療，包括基本創傷處理、基層醫療服務的持續與重建、公共衛生及環境維護、罹難者身分鑑識等。這些工作，不只需要醫療專業，也需要後勤、通訊、系統運作協調等，這些都亟需仰賴平時的演習訓練。 平常在電視上看到醫療救援人員在災難現場忙碌，少有機會了解他們為了「用在一時」而默默「養兵千日」的用心。 為了隨時可能發生的災難救援，早在921大地震前就出國進修緊急醫療的石富元經常到各地演講、利用假日舉辦教學與訓練，他開放給所有有興趣的民眾參加，「我不只需要醫護人員，災難時後勤工作更是重要，例如廚師、水電工、搬運人員等等，緊急應變時需要多功能的整合。」幾年下來，石富元陸續舉辦過多種緊急應變及管理的課程，單單災難救護隊的訓練就超過30多個梯次、訓練人數將近4000人。 當務之急，系統性的整合 經常穿梭在災難第一現場的石富元觀察發現，台灣人其實很熱心，只要一發生災難，救援物資、醫護人員、義工都會從四面八方湧入，可惜彼此之間缺乏協調與整合，而且熱情的時間很短，通常災難之後只剩少數人持續關切，對當地長期的幫助有限。 「災難醫學需要細火慢燉，但台灣人喜歡大火快炒，」石富元妙喻。 災難醫療救援不論在台灣或國外，都是一門正在發展中的新興醫學。 在美國，因1985年的墨西哥大地震，大批美國人前往救援，發現災難時的救援、救護、醫療等與平日的醫療工作哲學相當不同，才漸次發展出災難醫學。直到1989年與1997年兩次加州大地震、1995年美國奧克拉荷馬爆炸案，甚至於2001年的911事件，促使災難醫學及災難管理的重要性成為各界重視的焦點。 台灣也是因經歷了921大地震後，指揮及救援系統出現明顯漏洞，各界才開始正視災難應變及災難醫學，認知到建立國家災難醫療救援系統的標準化作業模式，實乃當務之急。畢竟，下一個災難，沒人知道何時會發生。 從醫療衛生的觀點，災難醫學是門多功能整合的醫療科學，包括了公共衛生、流行病學、創傷科、毒物科、感染科、急診醫學、重症醫學、兒內科、營養、社會復健與身心照護等整合性急難管理，需要建立系統整合。 「災難應變不是拿著圓鍬或十字鎬去救人而已，裡面還有很多專業學問，」石富元不禁提高語氣說，資源調度控管在災難現場，格外重要。 他經常在災難現場看到來自各地的醫療隊，各據一方，中間劃一條線，彷彿搶生意般做義診。台灣僅彈丸之大，進行緊急醫療救援時不需再區分中央政府或地方政府，所有的救援隊都屬於國家及社會的資源，唯一的工作就是人命救助，根本不分哪個國籍或種族，災難應變時不能容許有「貪天之功，以為己力」的想法，石富元認為。 目前，衛生署在全國設有六個緊急應變指揮中心，以台北區來說，範圍涵蓋了台北縣市、基隆與宜蘭，一旦發生災難，會進行需求評估，了解當地基本的衝擊與受損程度，必要時會進行各種應變處置，包括決定啟動國家級（levelⅠ）或地方級（levelⅡ）救援。 台灣重視「死傷數目」以及有多少人被困、被救援，反而不太重視災難對當地生活或基層醫療造成的長期破壞。例如2001年桃芝颱風肆虐，土石',\n",
       "    'keywords': ('訓練', '醫療', '災難', '登山意外', '義診', '救援'),\n",
       "    'publish_date': '20071101',\n",
       "    'title': '災難醫療救援訓練第一現場',\n",
       "    'url': 66663.0}],\n",
       "  'size': 1},\n",
       " {'keyword': '安養機構',\n",
       "  'member': [{'article_id': '45731a28-086d-4711-a09c-bafd680f4fac',\n",
       "    'author': '李宜芸',\n",
       "    'category': '照顧父母',\n",
       "    'content': '目前的檢討聲浪主要在「高樓層」阻礙逃生、救援困難，傳出衛生福利部研擬未來老人機構設置需要「限高」，限制高樓層不得設置機構。 「不能聚焦高樓層，而是重新規範高樓層機構的消防法規， 」雲林縣老人保護協會理事長林金立認為。 他指出，對於許多行動不便的長輩而言，「2樓就是『高樓層』，2樓跟8樓意思是一樣的，」排除高樓層設置不能解決現狀，況且許多醫院也是高樓層，也有許多行動不便的病患，是否醫院也要限高？ 他認為，焦點應放在機構的消防設備以及人員訓練。比如，「機構設置規範」規定，消防設備每3個月就要檢查、6個月就要演習一次，機構必須落實，不能落於徒具形式的演習。不過他也坦言，小型機構人力吃緊、離職率高，需要克服並強化人員應變的訓練。 另外，林金立也建議，每家機構都要有自衛消防演練計畫，甚至找消防專家量身打造每間機構的疏散計畫；各地的消防主管機關也需針對轄區高樓層機構有應變計劃，因為同樓層的機構，不同環境與地點，疏散計畫應該不同。 他進一步指出，雖然機構訂好計畫後會送到消防主管機關核備，他希望，消防主管機關不只是督導考核，而是到每個機構的現場消防演練出現的問題，提出修正意見，純粹的督察反而會讓缺點被掩蓋，不利改善現狀。 台灣老人暨長期照護社會工作專業協會理事長李梅英也認同，希望消防專業能夠介入長照相關機構，量身打造防災計畫。 她舉例，早期火災逃生觀念要求機構要垂直逃生（用救助袋垂吊），但在機構現場操作困難，因為長輩完全臥床、骨鬆等無法來得及應變，所以在長照機構中，更好的是水平逃生，設置消防防護區劃，當火災發生時，可到防火區劃爭取時間平安等待救援。 不過即便是水平逃生，小型機構可能還是有實施上的困難。實務狀況中，水平逃生需要一定的等待救援空間，對面積小的機構，因不清楚起火點時，要萬無一失，幾乎整間機構都要規劃成防火區劃，但防火門重又密閉，並不利平時長輩活動。 除了火災，台灣天然災害頻繁，還可能碰上颱風、水災、地震、旱災、土石流等，機構也要做足訓練，才能因應。 李梅英舉水災為例，目前水利署建議淹水30公分以上（因地制宜50公分）異地撤離。但「對長期臥床的長輩，如何『異地逃生』？還是只能往高樓層等待救援，平時就要計算出機構儲水及存量可支撐的天數，並注意夠不夠等待救援。」 「機構以往只能參考政府公布的消防防護等相關',\n",
       "    'keywords': ('樂活老人長照中心', '安養機構', '火災', '高樓層', '消防'),\n",
       "    'publish_date': '20160707',\n",
       "    'title': '安養機構大火／長照安全  重點不在限制高樓層',\n",
       "    'url': 72607.0},\n",
       "   {'article_id': '45731a28-086d-4711-a09c-bafd680f4fac',\n",
       "    'author': '李宜芸',\n",
       "    'category': '健康新知',\n",
       "    'content': '目前的檢討聲浪主要在「高樓層」阻礙逃生、救援困難，傳出衛生福利部研擬未來老人機構設置需要「限高」，限制高樓層不得設置機構。 「不能聚焦高樓層，而是重新規範高樓層機構的消防法規， 」雲林縣老人保護協會理事長林金立認為。 他指出，對於許多行動不便的長輩而言，「2樓就是『高樓層』，2樓跟8樓意思是一樣的，」排除高樓層設置不能解決現狀，況且許多醫院也是高樓層，也有許多行動不便的病患，是否醫院也要限高？ 他認為，焦點應放在機構的消防設備以及人員訓練。比如，「機構設置規範」規定，消防設備每3個月就要檢查、6個月就要演習一次，機構必須落實，不能落於徒具形式的演習。不過他也坦言，小型機構人力吃緊、離職率高，需要克服並強化人員應變的訓練。 另外，林金立也建議，每家機構都要有自衛消防演練計畫，甚至找消防專家量身打造每間機構的疏散計畫；各地的消防主管機關也需針對轄區高樓層機構有應變計劃，因為同樓層的機構，不同環境與地點，疏散計畫應該不同。 他進一步指出，雖然機構訂好計畫後會送到消防主管機關核備，他希望，消防主管機關不只是督導考核，而是到每個機構的現場消防演練出現的問題，提出修正意見，純粹的督察反而會讓缺點被掩蓋，不利改善現狀。 台灣老人暨長期照護社會工作專業協會理事長李梅英也認同，希望消防專業能夠介入長照相關機構，量身打造防災計畫。 她舉例，早期火災逃生觀念要求機構要垂直逃生（用救助袋垂吊），但在機構現場操作困難，因為長輩完全臥床、骨鬆等無法來得及應變，所以在長照機構中，更好的是水平逃生，設置消防防護區劃，當火災發生時，可到防火區劃爭取時間平安等待救援。 不過即便是水平逃生，小型機構可能還是有實施上的困難。實務狀況中，水平逃生需要一定的等待救援空間，對面積小的機構，因不清楚起火點時，要萬無一失，幾乎整間機構都要規劃成防火區劃，但防火門重又密閉，並不利平時長輩活動。 除了火災，台灣天然災害頻繁，還可能碰上颱風、水災、地震、旱災、土石流等，機構也要做足訓練，才能因應。 李梅英舉水災為例，目前水利署建議淹水30公分以上（因地制宜50公分）異地撤離。但「對長期臥床的長輩，如何『異地逃生』？還是只能往高樓層等待救援，平時就要計算出機構儲水及存量可支撐的天數，並注意夠不夠等待救援。」 「機構以往只能參考政府公布的消防防護等相關',\n",
       "    'keywords': ('樂活老人長照中心', '安養機構', '火災', '高樓層', '消防'),\n",
       "    'publish_date': '20160707',\n",
       "    'title': '安養機構大火／長照安全  重點不在限制高樓層',\n",
       "    'url': 72607.0}],\n",
       "  'size': 2},\n",
       " {'keyword': '醫療',\n",
       "  'member': [{'article_id': '065ece36-6a2e-4059-8a96-d09e90c9bc63',\n",
       "    'author': '林芝安',\n",
       "    'category': '醫療迷思',\n",
       "    'content': '清晨6點不到，所有團員在駐紮一晚的蒙古包外面集合，壓緊帽子、裹著禦寒大衣，逐一清點好相關急救醫療物品後，開始往看來不算艱困的山上挺進。 沒多久，才發現這座山會騙人，因為坡度不小，中途得休息好幾回，除了體力過人的領隊唐高駿一馬當先之外，大夥無不氣喘吁吁，餓著肚子繼續攻堅。在陣陣冷風中，總算攀上山頭，準備進行災難醫療救援訓練（Disaster Medical Assistant Team , DMAT）。 大夥各自就定位，有人扮演傷患、有人負責通報、有人忙著綁繩結、還有些人負責製作擔架……隨著分秒流逝，突然，朝陽從遙遠山頭探出臉來，頃刻間，第一道曙光射出，金色光芒遍灑山頭，然後將腳下的大地染成黃紅一片，在山上忙個不停的我們，有如沐浴在光中。 除了義診工作，接受災難醫療救援訓練也是本次義診團的主要任務之一，所有醫護人員都必須參與演習。 當災難發生時，如何在最短的時間內進行緊急救護與醫療照顧，將傷害減至最低，平時的訓練與演習就變得很重要，尤其台灣，地震、水災等各種災難頻繁，一旦發生重大災難，醫療救援隊必須緊急進駐，協助災區復原。 台北市立聯合醫院災難醫療救援隊利用這次外蒙義診，培訓醫護人員為種子志工，進行野外訓練。 就地取材，用枯木做擔架 台北市立聯合醫院忠孝院區院長兼總院區緊急醫療部部長唐高駿說，難得有機會移師外蒙古山區，可練習如何就地取材，利用當地既有的資源進行應變與救援。說著說著，唐高駿轉身往山坡樹林方向跑，沒多久，他肩上扛著長條白樺樹枯木回來，利用兩根木頭將三件救援背心串起來，就完成了克難擔架。 模擬事件：登山意外 醫護人員趕抵現場，檢傷後得知，一位重傷（疑似脊椎受傷）、兩位輕傷（一位手傷，一位腳傷）。 先在山上對重傷病患進行頸椎固定，使用繩結將患者綑綁在擔架上，初步評估有無心跳、血壓後，趕緊利用無線通報醫療站人員待命，之後用枯木做成的擔架將重傷患者搬運下山，另外兩位輕傷患者則用三角巾簡單固定傷口，由醫護人員陪同，緩步下山。 一抵達山下醫療站（設於蒙古包內），待命人員隨即對重傷患者進行高級創傷救命術（ATLS）及高級心臟救命術（ACLS）的評估，先評估病患的呼吸道是否順暢，再測量生命徵象（血壓、血氧），給予氧氣，建立靜脈輸液管路，並做神經學檢查，並針對緊急的問題做處理，穩定病患的生命徵象。 確認病患生命徵象穩定後，立即以無線電通報指揮派遣中心，連絡後送事宜。接著，將重傷病患送上救護車，緊急送往就近的醫院處置。 這是我們在外蒙古特勒吉山區的演練過程。充當重傷病患的我初體驗被綑綁在克難擔架上，由兩位急救人員抬下山，躺在擔架上不太舒服，腰背很痛，抬的人恐怕更辛苦，因為邊扛邊下坡。 為了這次外蒙訓練，出國前一星期的週末，唐高駿派義診成員忠孝院區護理科督導魏惠志與和平院區護士鄭雪琴，前往南投參加北區國家災難醫療救護隊野外模擬演習。 課程非常多元，包括如何搭設野外醫療站、檢傷分類及創傷處理、醫療站規劃設計、衛星定位系統（GPS）的使用、衛星電話的操作、繩結操作、野外求生及傷病患緊急醫療處置綜合演練等。 南投演習訓練的總指揮官是台大醫院急診醫學部主治醫師、台北區災難應變指揮中心（EOC）執行長石富元，他強調，災難醫學的重點是「緊急應變準備」，最重要的工作是準備，就像球隊，平常若沒勤奮練習，比賽前即使請國際級教練親臨指導也沒用。 災難醫療救援訓練也是如此，921大地震、風災、水災或SARS期間，醫療衛生體系都必須要做一些應變及進行初期救援或是醫療，包括基本創傷處理、基層醫療服務的持續與重建、公共衛生及環境維護、罹難者身分鑑識等。這些工作，不只需要醫療專業，也需要後勤、通訊、系統運作協調等，這些都亟需仰賴平時的演習訓練。 平常在電視上看到醫療救援人員在災難現場忙碌，少有機會了解他們為了「用在一時」而默默「養兵千日」的用心。 為了隨時可能發生的災難救援，早在921大地震前就出國進修緊急醫療的石富元經常到各地演講、利用假日舉辦教學與訓練，他開放給所有有興趣的民眾參加，「我不只需要醫護人員，災難時後勤工作更是重要，例如廚師、水電工、搬運人員等等，緊急應變時需要多功能的整合。」幾年下來，石富元陸續舉辦過多種緊急應變及管理的課程，單單災難救護隊的訓練就超過30多個梯次、訓練人數將近4000人。 當務之急，系統性的整合 經常穿梭在災難第一現場的石富元觀察發現，台灣人其實很熱心，只要一發生災難，救援物資、醫護人員、義工都會從四面八方湧入，可惜彼此之間缺乏協調與整合，而且熱情的時間很短，通常災難之後只剩少數人持續關切，對當地長期的幫助有限。 「災難醫學需要細火慢燉，但台灣人喜歡大火快炒，」石富元妙喻。 災難醫療救援不論在台灣或國外，都是一門正在發展中的新興醫學。 在美國，因1985年的墨西哥大地震，大批美國人前往救援，發現災難時的救援、救護、醫療等與平日的醫療工作哲學相當不同，才漸次發展出災難醫學。直到1989年與1997年兩次加州大地震、1995年美國奧克拉荷馬爆炸案，甚至於2001年的911事件，促使災難醫學及災難管理的重要性成為各界重視的焦點。 台灣也是因經歷了921大地震後，指揮及救援系統出現明顯漏洞，各界才開始正視災難應變及災難醫學，認知到建立國家災難醫療救援系統的標準化作業模式，實乃當務之急。畢竟，下一個災難，沒人知道何時會發生。 從醫療衛生的觀點，災難醫學是門多功能整合的醫療科學，包括了公共衛生、流行病學、創傷科、毒物科、感染科、急診醫學、重症醫學、兒內科、營養、社會復健與身心照護等整合性急難管理，需要建立系統整合。 「災難應變不是拿著圓鍬或十字鎬去救人而已，裡面還有很多專業學問，」石富元不禁提高語氣說，資源調度控管在災難現場，格外重要。 他經常在災難現場看到來自各地的醫療隊，各據一方，中間劃一條線，彷彿搶生意般做義診。台灣僅彈丸之大，進行緊急醫療救援時不需再區分中央政府或地方政府，所有的救援隊都屬於國家及社會的資源，唯一的工作就是人命救助，根本不分哪個國籍或種族，災難應變時不能容許有「貪天之功，以為己力」的想法，石富元認為。 目前，衛生署在全國設有六個緊急應變指揮中心，以台北區來說，範圍涵蓋了台北縣市、基隆與宜蘭，一旦發生災難，會進行需求評估，了解當地基本的衝擊與受損程度，必要時會進行各種應變處置，包括決定啟動國家級（levelⅠ）或地方級（levelⅡ）救援。 台灣重視「死傷數目」以及有多少人被困、被救援，反而不太重視災難對當地生活或基層醫療造成的長期破壞。例如2001年桃芝颱風肆虐，土石',\n",
       "    'keywords': ('訓練', '醫療', '災難', '登山意外', '義診', '救援'),\n",
       "    'publish_date': '20071101',\n",
       "    'title': '災難醫療救援訓練第一現場',\n",
       "    'url': 66663.0}],\n",
       "  'size': 1},\n",
       " {'keyword': '基改食品',\n",
       "  'member': [{'article_id': '80b59564-46b0-4b6e-81f6-67a659c19048',\n",
       "    'author': '王暄茹',\n",
       "    'category': '營養新知',\n",
       "    'content': '莎弗番茄是美國加州一家小生技公司製造出來的番茄，在1994年上市時，還驕傲地標明自己的身份是「世界上第一個基因改造食品」，雖然價格昂貴，試吃者評價不一，還是供不應求，銷售一空。 當時，基改食品提供無限想像，以為農民不用再煩惱蟲害，殺蟲劑能少用點，能守護消費者的健康，還被認為是難民救星，可以解決全球糧荒。 「當面臨自然環境變遷、全球糧荒等困境，人類必須找到能克服逆境的作物，有時基因改造技術能做到傳統育種所不能的事，」中央研究院農業生物科技研究中心主任施明哲指出。 所謂基改，是指透過生物科技轉殖基因，改變生物的性狀，在農業上可增加作物對抗自然逆境、避免害蟲、乾旱、水災的威脅，降低生產成本，也提高作物產量。 目前的基因改造作物可分為耐除草劑、抗蟲害、抗病害及提高營養價值四大類，且從單一轉殖基因進入多重轉殖基因，如同時抗除草劑和抗蟲害，一種作物可堆疊5～6種基因品系。 台灣本土開發的基改農產品還有抗旱及抗低溫水稻、抗鹽番茄、耐寒馬鈴薯、抑制老化基因的甘藷等抗自然逆境的產品，但都處於實驗室或田間試驗階段，尚未准許商業種植。 基改產品的發展也愈趨多元。2003年台灣發表的基改螢光魚也登上國際舞台；而加拿大開發的基改鮭魚可縮短二分之一養殖期，一年半的鮭魚體型就和三年的一樣大，大大降低養殖漁業的生產成本。 但近年來，當基改不斷挑戰未知，消費者對基改食品的疑慮聲浪打破了過去的美好想像。 今年5月24日共54個國家、400個城市（包括台灣）響應由美國猶他州一位母親卡諾（Tami Monroe Canal）所發起的反基改遊行，呼籲大眾正視基改食品的長期風險，她說：「基改食品毒害兒童、毒害地球。」她原本以為3000人上街就算成功，沒想到最後成為全球超過200萬人的大規模示威遊行。 由於環保意識高漲，國際上對基改食品憂慮更深。從抗蟲的蛋白恐引發過敏、抗除草劑的玉米增加腫瘤及死亡機會的個案及研究報告陸續曝光，身為母親的消費者更擔心孩子因而生病。生態學家則憂心基改作物破壞生態平衡，超級雜草和超級害蟲可能都與基改作物有關。 包括孟山都、拜耳、先正達在內的基改種子公司因此被視為科學怪物食物的發明者、危害世人健康及環境生態的兇手。消費者對種子公司進',\n",
       "    'keywords': ('基改食品', '基因改造', '基改原料', '消費者'),\n",
       "    'publish_date': '20140701',\n",
       "    'title': '基改vs.反基改勢力展開全球攻防',\n",
       "    'url': 68566.0}],\n",
       "  'size': 1},\n",
       " {'keyword': '台灣',\n",
       "  'member': [{'article_id': 'ad20ca7a-88ea-417b-bae9-f2b2c4b1b9c6',\n",
       "    'author': '李偉文',\n",
       "    'category': '專欄',\n",
       "    'content': '台灣像個大型樹木博物館 我順便跟A、B寶介紹一下台灣的森林。因為地形、緯度及季風氣候的關係，台灣溫熱多雨，非常適合樹木生長，再加上海拔高度落差非常大，以及位於陸地與海洋的交界，所以台灣的樹木種類非常多，除了來自大陸北方，也有來自南洋各群島，同時也有屬於寒帶的樹木，也有屬於熱帶赤道地區的樹種。住在台灣這麼小的島嶼，就像在逛座大型的樹木博物館一樣紛繽有趣。 我問她們：「為什麼水源區的森林維護做得特別嚴格？」 小時候曾經在荒野炫蜂團報告過的A寶很快地回答：「因為森林是水的故鄉，大樹有很多樹葉，森林裏有各種不同的樹木高高低低的，再加上攀爬的藤蔓類，這麼多層次的結構，可以把雨水留下來，而且樹根在土壤裏面，可以將水儲存在土壤深處，慢慢排出。」 B寶接著說：「森林就像一塊巨大的海棉，可以吸收並且過濾雨水，因此有森林的地方，就會有豐富與乾淨的水源。」 我讚許地鼓鼓掌，繼續說：「的確，除了森林可以調節降雨之外，樹木還可以提供我們果實木材，森林在穩定氣候清淨空氣，吸收二氧化碳，產生氧氣，還會防止土石流，山洪爆發，一方面使人類免於水災，一方面也使我們免於乾旱之苦，一棵樹等於一個小小的生態系統，有許多藤蔓、蕨類與蕈類都依賴著樹存活，還有數不盡的微',\n",
       "    'keywords': ('翡翠水庫', '樹木', '家具', '台灣', '森林', '木頭'),\n",
       "    'publish_date': '20120101',\n",
       "    'title': '有樹，就有希望',\n",
       "    'url': 65488.0}],\n",
       "  'size': 1}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(clusters_json, open('test.json', 'w'))#, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##########################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#word_vectors = [w2v_model[key] for key in w2v_model.vocab if key in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sumVectors(words):\n",
    "    sentence_vector = []\n",
    "    seg = jieba.lcut(comment)\n",
    "    for w in words:\n",
    "        if w in w2v_model.vocab:\n",
    "            sentence_vector.append(w2v_model[w])\n",
    "    return np.sum(sentence_vector, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_comment_tag['vector'] = df_comment_tag['seg_comment'].apply(sumVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agglo_model = AgglomerativeClustering(n_clusters=1500, affinity='cosine', linkage='complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot extract more clusters than samples: 1500 clusters where given for a tree with 1000 leaves.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-397-5828b27f1177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magglo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_comment_tag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vector'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jlin/.pyenv/versions/3.5.2/envs/kanjian/lib/python3.5/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# non-optimized default implementation; override when a better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# method is possible for a given clustering algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jlin/.pyenv/versions/3.5.2/envs/kanjian/lib/python3.5/site-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompute_full_tree\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             self.labels_ = _hc_cut(self.n_clusters, self.children_,\n\u001b[0;32m--> 738\u001b[0;31m                                    self.n_leaves_)\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_hierarchical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhc_get_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jlin/.pyenv/versions/3.5.2/envs/kanjian/lib/python3.5/site-packages/sklearn/cluster/hierarchical.py\u001b[0m in \u001b[0;36m_hc_cut\u001b[0;34m(n_clusters, children, n_leaves)\u001b[0m\n\u001b[1;32m    562\u001b[0m         raise ValueError('Cannot extract more clusters than samples: '\n\u001b[1;32m    563\u001b[0m                          \u001b[0;34m'%s clusters where given for a tree with %s leaves.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m                          % (n_clusters, n_leaves))\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;31m# In this function, we store nodes as a heap to avoid recomputing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;31m# the max of the nodes: the first element is always the smallest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot extract more clusters than samples: 1500 clusters where given for a tree with 1000 leaves."
     ]
    }
   ],
   "source": [
    "predicted_labels = agglo_model.fit_predict(df_comment_tag['vector'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y3 = w2v_model.most_similar(['治療'], topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('藥物治療', 0.7594596743583679), ('治療方式', 0.7414567470550537), ('手術', 0.7131774425506592), ('手術治療', 0.6928289532661438), ('療法', 0.689993143081665), ('癌症治療', 0.6893002986907959), ('治療方法', 0.6822962164878845), ('化學治療', 0.6813525557518005), ('中醫治療', 0.6758089065551758), ('化療', 0.6729283332824707), ('用藥', 0.6710823774337769), ('復發', 0.66049724817276), ('診斷', 0.6587794423103333), ('早期診斷', 0.653386116027832), ('處置', 0.6483652591705322), ('病程', 0.642364501953125), ('診療', 0.6410486698150635), ('治癒', 0.637911319732666), ('標靶藥物', 0.6375629901885986), ('放射線治療', 0.6278493404388428), ('西醫', 0.6252554655075073), ('標靶治療', 0.6243441104888916), ('開刀', 0.6167346835136414), ('肝癌', 0.6061704754829407), ('復健', 0.6058024168014526), ('急性期', 0.6047356724739075), ('診治', 0.6030184030532837), ('晚期', 0.6015010476112366), ('術後', 0.6006958484649658), ('療程', 0.5985381603240967), ('根治', 0.5973528027534485), ('放射治療', 0.595122218132019), ('洗腎', 0.593602180480957), ('放血', 0.589836597442627), ('病情', 0.5884119272232056), ('肺癌', 0.5877577066421509), ('早期', 0.5867290496826172), ('病因', 0.5862845182418823), ('服藥', 0.5850120186805725), ('針灸', 0.5836048722267151), ('急救', 0.5823644995689392), ('癌症病人', 0.5767701864242554), ('藥物', 0.5747889280319214), ('外科手術', 0.5739190578460693), ('介入', 0.5715091228485107), ('大腸直腸癌', 0.5663025975227356), ('甲狀腺癌', 0.5650857090950012), ('腹腔鏡手術', 0.5648874640464783), ('分期', 0.5636163949966431), ('麻醉', 0.5623922348022461)]\n"
     ]
    }
   ],
   "source": [
    "print(y3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
